apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "pipelines.name" . }}-pipelines-files
data:
  pipeline-requirements.txt: |
    requests
    psycopg2-binary
    llama-index
    llama-index-vector-stores-postgres
    llama-index-embeddings-openai-like
    llama-index-llms-openai-like
    llama-index-tools-mcp
    opencv-python-headless
    kubernetes
  rag-pipeline.py: | # pragma: allowlist secret
    import json
    import anyio
    from typing import Dict, Any
    from llama_index.tools.mcp import BasicMCPClient, McpToolSpec
    from llama_index.core.tools import QueryEngineTool, FunctionTool
    from llama_index.core.agent.workflow import ReActAgent
    from llama_index.core.workflow import Context, Event
    from llama_index.core import Settings, VectorStoreIndex, Document
    from llama_index.llms.openai_like import OpenAILike
    from llama_index.core.llms import ChatMessage
    from llama_index.embeddings.openai_like import OpenAILikeEmbedding
    from llama_index.vector_stores.postgres import PGVectorStore

    from kubernetes import client, config as k8s_config
    import base64


    class Pipeline:
      def __init__(self):
        self.name = "rag-pipeline"
        self.agent_config = None
        self.index = None
        self.agent = None

      async def on_startup(self):
        # Load config from mounted ConfigMap
        config_path = "/config/agent-config.json"
        with open(config_path, "r") as f:
          self.agent_config = json.load(f)

        print(f"DEBUG: Loaded agent config: {json.dumps(self.agent_config, indent=2)}")
        self.name = self.agent_config["name"]

        # Build agent from config
        await self._build_agent_from_config()

      async def _build_agent_from_config(self):
        # Load LLM from config
        llm = OpenAILike(
          model=self.agent_config["foundation_model"]["name"],
          api_base=f"http://{self.agent_config['foundation_model']['endpoint']}/openai/v1",
          max_tokens=self.agent_config.get("max_tokens", 512),
          is_chat_model=True,
        )

        Settings.llm = llm

        # Set up embedding model globally (assumes all KBs use the same embedding model)
        for tool_spec in self.agent_config.get("tools", []):
          if tool_spec.get("type") == "knowledgeBase" and "config" in tool_spec:
            kb_config = tool_spec["config"]

            embedding_model = kb_config.get("embedding_model")
            embedding_api_base = kb_config.get("embedding_api_base")

            if embedding_model and embedding_api_base:
              Settings.embed_model = OpenAILikeEmbedding(
                model_name=embedding_model,
                api_base=embedding_api_base,
                embed_batch_size=kb_config.get("embed_batch_size", 10),
                max_retries=3,
                timeout=180.0,
              )
              break  # Only set once from first KB

        # Dynamically build tools from config
        tools = []
        for tool_spec in self.agent_config.get("tools", []):
          tool_type = tool_spec.get("type")

          if tool_type == "knowledgeBase":
            # Build knowledge base tool if config is present
            if "config" in tool_spec:
              kb_config = tool_spec["config"]
              kb_name = tool_spec.get("name", "knowledge_base")

              index = self._build_vector_index(kb_config, kb_name)
              query_engine = index.as_query_engine(similarity_top_k=3)
              tool = QueryEngineTool.from_defaults(
                query_engine=query_engine,
                name=f"{kb_name}_knowledge_base",
                description=tool_spec.get("description", "Knowledge base search tool"),
              )
              tools.append(tool)

          elif tool_type == "function":
            # Function tool
            fn = self._resolve_function(tool_spec.get("name"))
            tool = FunctionTool.from_defaults(
              fn=fn,
              name=f"{tool_spec.get('name')}_function",
              description=tool_spec.get("description", "Function tool"),
            )
            tools.append(tool)

          elif tool_type == "mcpServer":
            # MCP server tool - skip if endpoint is unreachable
            try:
              mcp_client = BasicMCPClient(tool_spec.get("endpoint"))
              mcp_tool = McpToolSpec(client=mcp_client)
              tools.extend(await mcp_tool.to_tool_list_async())
            except Exception as e:
              print(f"Warning: Failed to connect to MCP server at {tool_spec.get('endpoint')}: {e}")

        # Enhance agent instructions to prevent looping
        base_instructions = self.agent_config.get("agent_instructions", "You are a helpful AI assistant.")
        agent_instructions = f"""{base_instructions}
        When answering questions:
        - Provide direct, concise answers
        - Only use tools when necessary to answer the question
        - Once you have gathered enough information, provide your final answer and STOP
        - Do not continue calling tools after you have the answer
        - Format your response clearly and concisely
        - Limit the number of tool calls to avoid infinite loops
        - Only output the answer in your final response, without extra commentary."""

        self.agent = ReActAgent(
          tools,
          llm=llm,
          verbose=True,
          system_prompt=agent_instructions,
        )
        self.ctx = Context(self.agent)

      def _build_vector_index(self, kb_config: Dict[str, Any], kb_name: str):
        """Builds a vector index based on KB config."""
        db_credentials = self._get_db_credentials(kb_config)

        vector_store = PGVectorStore.from_params(
          database=kb_name,  # Database name is the same as KB name
          host=db_credentials["host"],
          port=db_credentials["port"],
          user=db_credentials["username"],
          password=db_credentials["password"],
          table_name=kb_config.get("table_name"),
          embed_dim=int(kb_config.get("embed_dim")),
        )
        return VectorStoreIndex.from_vector_store(vector_store)

      def _resolve_function(self, fn_name: str):
        """Resolve function names to actual callables."""
        if fn_name == "web_search":
          return self._web_search
        raise ValueError(f"Unknown function {fn_name}")

      def _get_db_credentials(self, kb_config: Dict[str, Any]):
        """Get database credentials from Kubernetes secret."""
        k8s_config.load_incluster_config()
        v1 = client.CoreV1Api()
        secret = v1.read_namespaced_secret( # pragma: allowlist secret
          name=kb_config.get("secret_name"),
          namespace=kb_config.get("secret_namespace", self.agent_config["namespace"]),
        )
        return {
          "username": base64.b64decode(secret.data["username"]).decode("utf-8"),
          "password": base64.b64decode(secret.data["password"]).decode("utf-8"),
          "host": base64.b64decode(secret.data["host"]).decode("utf-8"),
          "port": int(base64.b64decode(secret.data["port"]).decode("utf-8")),
        }

      def _web_search(self, query: str) -> str:
        """Search the web for information."""
        ...

      def pipe(self, user_message, model_id, messages, body):
        response = self.agent.chat(user_message)

        # Stream the response
        for char in str(response):
            yield char
