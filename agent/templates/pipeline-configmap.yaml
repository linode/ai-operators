apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "pipelines.name" . }}-pipelines-files
data:
  pipeline-requirements.txt: |
    requests
    psycopg2-binary
    llama-index
    llama-index-vector-stores-postgres
    llama-index-embeddings-openai-like
    llama-index-llms-openai-like
    llama-index-tools-mcp
    opencv-python-headless
    kubernetes
  rag-pipeline.py: | # pragma: allowlist secret
    import json
    import anyio
    from typing import Dict, Any
    from llama_index.tools.mcp import BasicMCPClient, McpToolSpec
    from llama_index.core.tools import QueryEngineTool, FunctionTool
    from llama_index.core.agent.workflow import ReActAgent, AgentStream, AgentWorkflow, FunctionAgent, AgentInput, AgentOutput, ToolCall, ToolCallResult
    from llama_index.core.workflow import Context
    from llama_index.core import Settings, VectorStoreIndex, Document
    from llama_index.llms.openai_like import OpenAILike
    from llama_index.embeddings.openai_like import OpenAILikeEmbedding
    from llama_index.vector_stores.postgres import PGVectorStore

    from kubernetes import client, config as k8s_config
    import base64


    class Pipeline:
      def __init__(self):
        self.name = "rag-pipeline"
        self.agent_config = None
        self.index = None
        self.agent = None
        self.ctx = None

      async def on_startup(self):
        # Load config from mounted ConfigMap
        config_path = "/config/agent-config.json"
        with open(config_path, "r") as f:
          self.agent_config = json.load(f)

        print(f"DEBUG: Loaded agent config: {json.dumps(self.agent_config, indent=2)}")
        self.name = self.agent_config["name"]

        # Build agent from config
        await self._build_agent_from_config()

      async def _build_agent_from_config(self):
        # Load LLM from config
        llm = OpenAILike(
          model=self.agent_config["foundation_model"]["name"],
          api_base=f"http://{self.agent_config['foundation_model']['endpoint']}/openai/v1",
          max_tokens=self.agent_config.get("max_tokens", 512),
          is_chat_model=True,
        )

        Settings.llm = llm

        # Set up embedding model globally (assumes all KBs use the same embedding model)
        for tool_spec in self.agent_config.get("tools", []):
          if tool_spec.get("type") == "knowledgeBase" and "config" in tool_spec:
            kb_config = tool_spec["config"]

            embedding_model = kb_config.get("embedding_model")
            embedding_api_base = kb_config.get("embedding_api_base")

            if embedding_model and embedding_api_base:
              Settings.embed_model = OpenAILikeEmbedding(
                model_name=embedding_model,
                api_base=embedding_api_base,
                embed_batch_size=kb_config.get("embed_batch_size", 10),
                max_retries=3,
                timeout=180.0,
              )
              break  # Only set once from first KB

        # Dynamically build tools from config
        tools = []
        for tool_spec in self.agent_config.get("tools", []):
          tool_type = tool_spec.get("type")

          if tool_type == "knowledgeBase":
            # Build knowledge base tool if config is present
            if "config" in tool_spec:
              kb_config = tool_spec["config"]
              kb_name = tool_spec.get("name", "knowledge_base")

              index = self._build_vector_index(kb_config, kb_name)
              query_engine = index.as_query_engine(similarity_top_k=3)
              tool = QueryEngineTool.from_defaults(
                query_engine=query_engine,
                name=f"{kb_name}_knowledge_base",
                description=tool_spec.get("description", "Knowledge base search tool"),
              )
              tools.append(tool)

          elif tool_type == "function":
            # Function tool
            fn = self._resolve_function(tool_spec.get("name"))
            tool = FunctionTool.from_defaults(
              fn=fn,
              name=f"{tool_spec.get('name')}_function",
              description=tool_spec.get("description", "Function tool"),
            )
            tools.append(tool)

          elif tool_type == "mcpServer":
            # MCP server tool - skip if endpoint is unreachable
            try:
              mcp_client = BasicMCPClient(tool_spec.get("endpoint"))
              mcp_tool = McpToolSpec(client=mcp_client)
              tools.extend(await mcp_tool.to_tool_list_async())
            except Exception as e:
              print(f"Warning: Failed to connect to MCP server at {tool_spec.get('endpoint')}: {e}")

        # Enhance agent instructions to prevent looping
        base_instructions = self.agent_config.get("agent_instructions", "You are a helpful AI assistant.")
        agent_instructions = f"""{base_instructions}

          IMPORTANT RULES:
          - ONLY use tools when the user's question specifically requires information from the knowledge base
          - For general questions, greetings, or questions you can answer directly, respond WITHOUT using any tools
          - Use tools ending with _knowledge_base ONLY when the question asks about specific documentation, data, or domain knowledge
          - Do not call tools multiple times - use them once and provide a final answer"""

        if not tools:
          # No tools - just use LLM directly
          print("No tools configured, using LLM directly")
          self.agent = llm
          self.system_prompt = base_instructions
          self.ctx = None
        else:
          # Has tools - use AgentWorkflow
          print(f"Configured {len(tools)} tools, using AgentWorkflow")
          self.agent = AgentWorkflow.from_tools_or_functions(
            tools,
            llm=llm,
            verbose=True,
            system_prompt=agent_instructions,
            timeout=120.0,
          )
          self.ctx = Context(self.agent)
          self.system_prompt = None

      def _build_vector_index(self, kb_config: Dict[str, Any], kb_name: str):
        """Builds a vector index based on KB config."""
        db_credentials = self._get_db_credentials(kb_config)

        vector_store = PGVectorStore.from_params(
          database=kb_config.get("table_name"),  # Database name is the same as KB name
          host=db_credentials["host"],
          port=db_credentials["port"],
          user=db_credentials["username"],
          password=db_credentials["password"],
          table_name=kb_config.get("table_name"),
          embed_dim=int(kb_config.get("embed_dim")),
        )
        return VectorStoreIndex.from_vector_store(vector_store)

      def _resolve_function(self, fn_name: str):
        """Resolve function names to actual callables."""
        if fn_name == "web_search":
          return self._web_search
        raise ValueError(f"Unknown function {fn_name}")

      def _get_db_credentials(self, kb_config: Dict[str, Any]):
        """Get database credentials from Kubernetes secret."""
        k8s_config.load_incluster_config()
        v1 = client.CoreV1Api()
        secret = v1.read_namespaced_secret( # pragma: allowlist secret
          name=kb_config.get("secret_name"),
          namespace=kb_config.get("secret_namespace", self.agent_config["namespace"]),
        )
        return {
          "username": base64.b64decode(secret.data["username"]).decode("utf-8"),
          "password": base64.b64decode(secret.data["password"]).decode("utf-8"),
          "host": base64.b64decode(secret.data["host"]).decode("utf-8"),
          "port": int(base64.b64decode(secret.data["port"]).decode("utf-8")),
        }

      def _web_search(self, query: str) -> str:
        """Search the web for information."""
        ...

      def pipe(self, user_message, model_id, messages, body):
        # Check if using LLM directly (no tools) or AgentWorkflow
        if hasattr(self, 'system_prompt') and self.system_prompt:
            # LLM direct mode - no tools - simple synchronous chat
            from llama_index.core.llms import ChatMessage
            msgs = [
                ChatMessage(role="system", content=self.system_prompt),
                ChatMessage(role="user", content=user_message)
            ]
            response = self.agent.stream_chat(msgs)

            # Stream character by character
            for token in response:
                yield str(token.delta)
        else:
            # AgentWorkflow mode - has tools - needs async handling
            async def get_agent_response():
                handler = self.agent.run(user_message, ctx=self.ctx)
                async for event in handler.stream_events():
                    if isinstance(event, AgentStream):
                        print(event.delta, end="", flush=True)
                    elif isinstance(event, AgentInput):
                        print("Agent input: ", event.input)  # the current input messages
                        print("Agent name:", event.current_agent_name)  # the current agent name
                    elif isinstance(event, AgentOutput):
                        print("Agent output: ", event.response)  # the current full response
                        print("Tool calls made: ", event.tool_calls)  # the selected tool calls, if any
                        print("Raw LLM response: ", event.raw)  # the raw llm api response
                    elif isinstance(event, ToolCallResult):
                        print("Tool called: ", event.tool_name)  # the tool name
                        print("Arguments to the tool: ", event.tool_kwargs)  # the tool kwargs
                        print("Tool output: ", event.tool_output)  # the tool output
                final_result = await handler
                return str(final_result)

            # Run async function and get response
            result = anyio.from_thread.run(get_agent_response)

            # Stream the response character by character with typing effect
            import time
            for char in result:
                yield char
                time.sleep(0.01)  # 10ms delay between characters for typing effect
